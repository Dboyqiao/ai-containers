ARG BASE_IMAGE_TAG="20.04"
# Inherit Python3
FROM ubuntu:${BASE_IMAGE_TAG}

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install --no-install-recommends --fix-missing -y \
    ca-certificates \
    git \
    libgomp1 \
    mpich \
    numactl \
    patch \
    wget

ARG CONDA_INSTALL_PATH=/opt/conda
ARG MINICONDA_VERSION="latest"
# Miniconda Installation
RUN apt-get update && \
    wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh -O miniconda.sh && \
    bash miniconda.sh -b -p ${CONDA_INSTALL_PATH} && \
    rm miniconda.sh && \
    ln -s ${CONDA_INSTALL_PATH}/etc/profile.d/conda.sh /etc/profile.d/conda.sh && \
    echo ". ${CONDA_INSTALL_PATH}/etc/profile.d/conda.sh" >> ~/.bashrc && \
    echo "conda activate dlsa" >> ~/.bashrc

ARG PYTHON_VERSION="3.7"
ENV PATH="${CONDA_INSTALL_PATH}/bin:${PATH}"

SHELL ["/bin/bash", "-c"]
# Create Conda Environment + Install dlsa reqs for Transformers
RUN conda create -yn dlsa python=${PYTHON_VERSION} && \
    source activate dlsa && \
    conda install -y -c conda-forge gperftools && \
    conda install -y intel-openmp pip astunparse numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses && \
    conda clean -ya && \
    pip install transformers==4.16.2 datasets==1.18.3 numpy
# Default Workspace
RUN mkdir -p /workspace

ENV LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/lib

ARG WHL_WW
ARG WHL_YR
# Get IPEX whl and install IPEX
RUN export PATH=${CONDA_INSTALL_PATH}/envs/dlsa/bin:${PATH} && \
    wget -r --no-parent -l1 --reject="index.html*" --cut-dirs=4 -nH http://mlpc.intel.com/downloads/cpu/${WHL_YR}/${WHL_YR}_ww${WHL_WW}/ -A "torch-*.whl" -P /tmp && \
    wget -r --no-parent -l1 --reject="index.html*" --cut-dirs=4 -nH http://mlpc.intel.com/downloads/cpu/${WHL_YR}/${WHL_YR}_ww${WHL_WW}/ -A "intel_extension_for_pytorch*.whl" -P /tmp && \
    pip install /tmp/torch-*.whl && \
    pip install /tmp/intel_extension_for_pytorch*.whl && \
    rm /tmp/torch-*.whl && \
    rm /tmp/intel_extension_for_pytorch*.whl
